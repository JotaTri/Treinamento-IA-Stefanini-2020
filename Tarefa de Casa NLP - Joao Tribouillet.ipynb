{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Tarefa de Casa NLP - Joao Tribouillet.ipynb","provenance":[{"file_id":"1G9lJsCkC7rQSShLXgVc0-Pb93uhn3yHI","timestamp":1582578687442}],"collapsed_sections":[],"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Bodtn5BX6a2x","colab_type":"text"},"source":["# Tarefa de Casa "]},{"cell_type":"markdown","metadata":{"id":"AqMU5ZR66a20","colab_type":"text"},"source":["O link abaixo disponibiliza um dataset de avaliações de filmes, contendo o texto da avaliação e se foi positiva ou negativa:\n","https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","\n","Fazer um pipeline completo de NLP para classificação com as seguintes características:\n","1 - Devem haver 3 classificadores, sendo pelo menos 1 de Deep Learning\n","2 - Justificativa de todos os artíficios utilizados, regex, modelos, camadas.\n","\n","Se concentrem na documentação e justificativa das escolhas. O importante é mostrar que entendeu o conteúdo através da pesquisa e não obter a melhor métrica.\n","\n","Caso tenham dúvidas:\n","leohb2@gmail.com\n","github.com/Leothi"]},{"cell_type":"code","metadata":{"id":"fPLpNL1ZDa0v","colab_type":"code","outputId":"8df6da1c-30a4-44fe-a30e-dde7aef27695","executionInfo":{"status":"ok","timestamp":1582756907876,"user_tz":180,"elapsed":2841,"user":{"displayName":"joão Tribouillet","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGS48rww8kY6ccKD-x-8pWuu_N_VfXMxxRFquLNA=s64","userId":"17416712450227202333"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import cv2\n","\n","import scipy\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","import spacy\n","import spacy.cli\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","LOCAL = False"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vM2J8RAvDcHP","colab_type":"code","colab":{}},"source":["from os import path\n","import zipfile\n","\n","if not path.exists('dataset'):\n","    link = 'https://drive.google.com/open?id=1lR9wldgNYUNN0HyPC8mBdltlzN4fox6m'\n","\n","    fluff, file_id = link.split('=')\n","    downloaded = drive.CreateFile({'id':file_id})\n","    downloaded.GetContentFile('dataset.zip')\n","\n","    archives = zipfile.ZipFile('dataset.zip')\n","\n","    archives.extractall('dataset')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-HKHmnCSERRy","colab_type":"code","outputId":"8670fac6-9952-437b-a56c-836a2f7d2e2e","executionInfo":{"status":"ok","timestamp":1582756908769,"user_tz":180,"elapsed":3707,"user":{"displayName":"joão Tribouillet","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGS48rww8kY6ccKD-x-8pWuu_N_VfXMxxRFquLNA=s64","userId":"17416712450227202333"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["df = pd.read_csv('dataset/IMDB Dataset.csv')\n","\n","df.info()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 2 columns):\n","review       50000 non-null object\n","sentiment    50000 non-null object\n","dtypes: object(2)\n","memory usage: 781.4+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YBT6bakHE4rk","colab_type":"code","outputId":"d6a902b3-eeae-4335-bb04-4d9969307a7f","executionInfo":{"status":"ok","timestamp":1582756908770,"user_tz":180,"elapsed":3693,"user":{"displayName":"joão Tribouillet","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGS48rww8kY6ccKD-x-8pWuu_N_VfXMxxRFquLNA=s64","userId":"17416712450227202333"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"source":["df.head(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"ZCKj6Lk2eIfH","colab_type":"text"},"source":["O bloco a seguir é responsável por limpar os textos presentes na coluna 'review'. Essa limpeza é necessária, visto que existem tags de html no meio do texto e para a melhor obtenção de informações sobre as palavras, deve-se retirar palavras que não agregam muita informação (stopwords) bem como retirar qualquer outro conjunto de caracteres que não sejam palavras, como acentos, pontuação etc.\n","\n","Alem disso, é retirada informações sobre o texto referente ao tamanho médio tanto de palavras quanto de sentenças de cada review do dataframe."]},{"cell_type":"code","metadata":{"id":"Eq9ap8oLG3mK","colab_type":"code","colab":{}},"source":["import unicodedata\n","import re\n","\n","\n","# Função responsável por limpar o texto e deixá-lo todo em lowercase\n","def clean_text(text):\n","    # Lista de stopwords em ingles\n","    en_stopwords = set(stopwords.words('english'))\n","    \n","    # Regex para retirada de 'não palavras'\n","    re_only_words = re.compile(r'\\W')\n","\n","    text = text.lower()\n","\n","    # Retirada de stopwords\n","    text = ' '.join(word for word in text.split() if word not in en_stopwords)\n","    \n","    # Retirada de acentos\n","    text = unicodedata.normalize('NFD', text)\n","    text = text.encode('ascii', 'ignore')\n","    text = text.decode(\"utf-8\")\n","\n","    # Retiradas de 'não palavras'\n","    text = re_only_words.sub(' ', text)\n","\n","\n","    # print(text)\n","    return text\n","\n","\n","# Função responsável pela retirdada de tags existentes dentro do texto\n","def clean_tags(text):\n","    re_tags = re.compile(r'<(.*?)>')\n","    \n","    # Retirada de tags\n","    text = re_tags.sub(' ', text)\n","    return text\n","\n","# Função responsável por obter o tamanho médio das palavras em caracteres\n","def avg_word(text):\n","    re_only_words = re.compile(r'\\W')\n","\n","\n","    text = re_only_words.sub(' ', text)\n","    total = 0\n","\n","    for word in text.split():\n","        total += len(word)\n","    return total/len(text.split())\n","\n","# Função responsável por obter o tamanho médio das sentenças em caracteres\n","def avg_sent(text):\n","    doc = nlp(text)\n","\n","    total = 0\n","    for sent in list(doc.sents):\n","        total += len(sent)\n","    return(total/len(list(doc.sents)))\n","\n","\n","# Apenas roda toda a parte de limpeza e obtenção de features caso a variável LOCAL esteja como Verdadeiro\n","# Caso contrário, um arquivo csv já com esses dados processados será utilizado mais a frente\n","if (LOCAL):\n","    print('Getting rid of tags...')\n","    df.review = df.review.apply(clean_tags)\n","\n","    print('Getting avg word length...')\n","    df['avg_word_lenght'] = df.review.apply(avg_word)\n","\n","    print('Getting avg sentences length...')\n","    df['avg_sent_lenght'] = df.review.apply(avg_sent)\n","\n","    print('Cleaning text...')\n","    df.review = df.review.apply(clean_text)\n","\n","    df.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yADK8iBIRNOz","colab_type":"code","colab":{}},"source":["# Adição das colunas referentes às features a serem obtidas do texto\n","features = ['ADJ', 'ADV', 'DET', 'NOUN', 'PRON', 'PROPN' ,'VERB', 'JJ', 'JJR', 'JJS']\n","for feature in features:\n","    df[feature] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSIocwasT-l_","colab_type":"code","outputId":"42d95776-ddd6-4815-c743-057ab7bda395","executionInfo":{"status":"ok","timestamp":1582756917485,"user_tz":180,"elapsed":12371,"user":{"displayName":"joão Tribouillet","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGS48rww8kY6ccKD-x-8pWuu_N_VfXMxxRFquLNA=s64","userId":"17416712450227202333"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"source":["import progressbar\n","from collections import Counter\n","\n","\n","# Preenchimento das novas colunas no dataframe com os sues respectivos valores\n","# (Apenas roda caso LOCAL seja True, caso contrário, baixa-se o .csv já processado)\n","if(LOCAL):\n","    removables = ['CCONJ', 'SPACE', 'NUM']\n","\n","    true_bar = progressbar.ProgressBar(maxval=df.shape[0], \\\n","    widgets=[progressbar.Bar('#', '[', ']'), '0/{}   '.format(df.shape[0], progressbar.Percentage)])\n","    true_bar.start()\n","    i = 0\n","\n","    for index, row in df.iterrows():\n","        i += 1\n","        true_bar.widgets[1] = ' {}/{}'.format(i,df.shape[0])\n","        true_bar.update(i)\n","\n","        doc = nlp(row.review)\n","        c = Counter(([token.pos_ for token in doc]))\n","        d = Counter(([token.tag_ for token in doc if token.pos_ == 'ADJ']))\n","        c.update(d)\n","\n","        for key in removables:\n","            del c[key]\n","        for key in c:\n","            row[key] = c[key]\n","        df.iloc[index] = row\n","    df.to_csv('IMDB Features.csv', index=False)\n","else:\n","  features_csv = 'https://drive.google.com/open?id=1qq1nXCQ9o6LMW1c8LjkH7gCtBWQKel1E'\n","\n","  file_id = features_csv.split('=')[1]\n","  downloaded = drive.CreateFile({'id':file_id})\n","  downloaded.GetContentFile('IMDB Features.csv')\n","\n","\n","df = pd.read_csv('IMDB Features.csv')\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","      <th>avg_word_lenght</th>\n","      <th>avg_sent_lenght</th>\n","      <th>ADJ</th>\n","      <th>ADV</th>\n","      <th>DET</th>\n","      <th>NOUN</th>\n","      <th>PRON</th>\n","      <th>PROPN</th>\n","      <th>VERB</th>\n","      <th>JJ</th>\n","      <th>JJR</th>\n","      <th>JJS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one reviewers mentioned watching 1 oz episode ...</td>\n","      <td>positive</td>\n","      <td>4.347134</td>\n","      <td>24.133333</td>\n","      <td>32</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>75</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>42</td>\n","      <td>29</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wonderful little production  filming technique...</td>\n","      <td>positive</td>\n","      <td>4.881250</td>\n","      <td>20.555556</td>\n","      <td>15</td>\n","      <td>13</td>\n","      <td>2</td>\n","      <td>38</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>thought wonderful way spend time hot summer we...</td>\n","      <td>positive</td>\n","      <td>4.244048</td>\n","      <td>32.500000</td>\n","      <td>21</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>38</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>21</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>basically there s family little boy  jake  thi...</td>\n","      <td>negative</td>\n","      <td>4.125926</td>\n","      <td>17.444444</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>33</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>petter mattei s  love time money  visually stu...</td>\n","      <td>positive</td>\n","      <td>4.456140</td>\n","      <td>21.500000</td>\n","      <td>23</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>63</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>23</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment  ...  JJR  JJS\n","0  one reviewers mentioned watching 1 oz episode ...  positive  ...    3    0\n","1  wonderful little production  filming technique...  positive  ...    0    0\n","2  thought wonderful way spend time hot summer we...  positive  ...    0    0\n","3  basically there s family little boy  jake  thi...  negative  ...    1    0\n","4  petter mattei s  love time money  visually stu...  positive  ...    0    1\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"0iNyz6pbfnDQ","colab_type":"text"},"source":["O bloco de código a seguir é responsável por stemizar e lematizar o texto da coluna 'review' já limpado. Isso ajuda algorítmos a extrair informações de texto, visto que retira informações temporais ou de concordância das palavras, permitindo que o texto final seja o mais cru e auditavel possível. "]},{"cell_type":"code","metadata":{"id":"X5aAfdvoyxv6","colab_type":"code","outputId":"b6e7c2dc-ebc2-403a-baaf-9055b640928a","executionInfo":{"status":"error","timestamp":1582757119675,"user_tz":180,"elapsed":75108,"user":{"displayName":"joão Tribouillet","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAGS48rww8kY6ccKD-x-8pWuu_N_VfXMxxRFquLNA=s64","userId":"17416712450227202333"}},"colab":{"base_uri":"https://localhost:8080/","height":449}},"source":["from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","# Função que retorna o texto stemizado\n","def get_stemmed_text(text):\n","    stemmer = PorterStemmer()\n","    text = ' '.join([stemmer.stem(word) for word in text.split()])\n","    return text\n","\n","# Função que retorna o texto lematizar\n","def get_lemmatized_text(text):\n","    lemmatizer = WordNetLemmatizer()\n","    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n","    return text\n","\n","\n","df.review = df.review.apply(get_stemmed_text)\n","df.review = df.review.apply(get_lemmatized_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-57deea7ea9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_stemmed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lemmatized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-9-57deea7ea9ca>\u001b[0m in \u001b[0;36mget_stemmed_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_stemmed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-57deea7ea9ca>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_stemmed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step1a\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# this NLTK-only rule extends the original algorithm, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# that 'flies'->'fli' but 'dies'->'die' etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ies'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ies'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ie'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"iKMzKmkGAEv2","colab_type":"code","colab":{}},"source":["df['clean review'] = df.review\n","df.drop(['review'], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B01AgS5cGtMF","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"shh46cY8A7IS","colab_type":"code","colab":{}},"source":["# Função responsável por codificar com um valor numérico o 'sentiment' de cada review do dataset\n","def code_sentiment(sentiment):\n","    if (sentiment == 'positive'):\n","        return 1\n","    return 0\n","\n","df.sentiment = df.sentiment.apply(code_sentiment)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBHFqwdyst8-","colab_type":"code","colab":{}},"source":["# import seaborn as sns\n","\n","# sns.pairplot(df.drop(['review'],axis=1), hue='sentiment')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaAs4o26BGRb","colab_type":"code","colab":{}},"source":["df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXBeJwexJua7","colab_type":"code","colab":{}},"source":["df.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"giHLpH1jgIvp","colab_type":"text"},"source":["A partir desse novo dataframe apenas com o review lematizado e stemizado, é feito a contagem das 1500 palavras que mais ocorrem bem como o TF-IDF das 1500 palavras com maior valor quando levado em conta todos os campos da coluna 'clean review'"]},{"cell_type":"code","metadata":{"id":"OrduTrdKIGAF","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","cv = CountVectorizer(binary=True, max_features=1500)\n","cv.fit(df['clean review'])\n","vectorizer = cv.transform(df['clean review'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfVJoiDHc_yh","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tv = TfidfVectorizer(\n","                    sublinear_tf = True,\n","                    max_features = 1500)\n","\n","train_tv = tv.fit_transform(df['clean review'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_C1dhE9dq7z","colab_type":"code","colab":{}},"source":["train_tv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujAekj20eE_P","colab_type":"code","colab":{}},"source":["# dist = np.sum(train_tv, axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Idzs3m0eKjA","colab_type":"code","colab":{}},"source":["tfidf_df = pd.DataFrame.sparse.from_spmatrix(train_tv, columns=tv.get_feature_names())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7Qw3qmxD-hN","colab_type":"code","colab":{}},"source":["count_df = pd.DataFrame.sparse.from_spmatrix(vectorizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxKLlK1z8vT4","colab_type":"code","colab":{}},"source":["df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mI3GauLNyUj","colab_type":"code","colab":{}},"source":["# Junção dos dois dataframes criados para o CountVectorizer e o TFIDFVectorizer com o dataframe original\n","# em um novo dataframe\n","new_df = df.join(count_df)\n","new_df = new_df.join(tfidf_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"14sRYWIhDAIZ","colab_type":"code","colab":{}},"source":["new_df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwiL1uIRDEFa","colab_type":"code","colab":{}},"source":["new_df.fillna(0, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojd86ZowhGlc","colab_type":"text"},"source":["A partir desse novo dataframe, podemos enfim separá-lo em treino e teste, instanciar um modelo e testá-lo. Isso é feito nos blocos de código a seguir."]},{"cell_type":"code","metadata":{"id":"zsLgN1XBAxmu","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, x_test, Y_train, y_test = train_test_split(new_df.drop(['sentiment', 'clean review'], axis=1), new_df['sentiment'], test_size=0.3, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rk1GLh0siUwo","colab_type":"text"},"source":["### Random Forest Classifier\n","\n","Escolheu-se iniciar com o modelo de Random Forest. A seguir, apresenta-se a criação do modelo, e o uso dos conjuntos de treino e teste para o seu treino e sua validação."]},{"cell_type":"code","metadata":{"id":"0DFyrHhgBKS7","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Criação do dataframe para o modelo com todos os parâmetros default\n","model = RandomForestClassifier(random_state=42)\n","\n","model.fit(X_train, Y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEt35SCrBSxl","colab_type":"code","colab":{}},"source":["# Obtenção da predição a partir do conjunto de teste com o modelo criado\n","pred = model.predict(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtJj--yDBXWH","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import recall_score, precision_score, accuracy_score\n","\n","matrix = confusion_matrix(y_test, pred)\n","\n","fig, ax = plt.subplots()\n","\n","sns.heatmap(matrix, annot=True, fmt='d')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","fig.show()\n","\n","# Verificação da accuracy, recall e precision do modelo a partir da predição com valores de teste\n","print('Model Accuracy:\\t\\t', accuracy_score(y_test, pred))\n","print('Model Recall:\\t\\t', recall_score(y_test, pred))\n","print('Model Precision:\\t', precision_score(y_test, pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iz3C1RUonEby","colab_type":"text"},"source":["Foi escolhido o RandomForestClassifier por ser muito adaptável a diversos tipos de problemas de classificação. os valores obtidos expostos acima se mostram favoráveis, com todos os parâmetros acima de 80%."]},{"cell_type":"markdown","metadata":{"id":"BGpdHIAwnSMa","colab_type":"text"},"source":["### Logistic Regression\n","\n","A seguir, escolheu-se aplicar o regessor logístico para a classificação. Nos blocos de códigos abaixo apresentam-se a criação do modelo, e o uso dos conjuntos de treino e teste para o seu treino e sua validação."]},{"cell_type":"code","metadata":{"id":"z2NVyiFPMUeu","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Criação do modelo com todos os parâmetros default\n","logistic_reg_clf = LogisticRegression(random_state=42)\n","\n","# Treino do modelo a partir do conjunto de dados separado para treino e suas labels\n","logistic_reg_clf.fit(X_train, Y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"67evWEAYMzfo","colab_type":"code","colab":{}},"source":["pred = logistic_reg_clf.predict(x_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYzraVCVM0QE","colab_type":"code","colab":{}},"source":["matrix = confusion_matrix(y_test, pred)\n","\n","fig, ax = plt.subplots()\n","\n","sns.heatmap(matrix, annot=True, fmt='d')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","fig.show()\n","\n","# Verificação da accuracy, recall e precision do modelo a partir da predição com valores de teste\n","print('Model Accuracy:\\t\\t', accuracy_score(y_test, pred))\n","print('Model Recall:\\t\\t', recall_score(y_test, pred))\n","print('Model Precision:\\t', precision_score(y_test, pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAB7BLoEnm0O","colab_type":"text"},"source":["Os resultados obtidos acima também foram favoráveis, até superiores do que os obtidos com o modelo de Random Forest. Sua aplicação para quantidade de variáveis adicionadas ao modelo parece lidar bem com esse modelo. "]},{"cell_type":"markdown","metadata":{"id":"zTCqTccUn7V8","colab_type":"text"},"source":["### Deep Learning\n","\n","A seguir, é apresentado a criação de um modelo de Deep Learning com 7 camadas."]},{"cell_type":"code","metadata":{"id":"oSMETslW7pxw","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","\n","# Criação de um modelo de uma rede do tipo Senquantial com 7 camadas\n","model = Sequential()\n","\n","model.add(Dense(units= 1024, activation='relu', input_shape=X_train.shape[1:]))\n","model.add(Dropout(rate=0.3))\n","model.add(Dense(units= 512, activation='relu'))\n","model.add(Dropout(rate=0.3))\n","model.add(Dense(units= 512, activation='relu'))\n","model.add(Dropout(rate=0.3))\n","model.add(Dense(units= 1, activation= 'sigmoid'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yJPMYRKOYVj","colab_type":"code","colab":{}},"source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","history = model.fit(X_train, Y_train, epochs=10, validation_data=(x_test, y_test), verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ict1Vc3wom0m","colab_type":"text"},"source":["Os valores obtidos acima foram também bons, com a acurácia acima de 86%. Entretanto, analisando sua evolução, é possível perceber que houve um pequeno overfit sobre o conjunto de treino. Sua evolução pode ser vista nos gráficos abaixo. Neles, verificamos que houve um aumento do *loss* de validação com as épocas."]},{"cell_type":"code","metadata":{"id":"rnVzgF9qmozC","colab_type":"code","colab":{}},"source":["from plotly.subplots import make_subplots\n","import plotly.graph_objects as go\n","\n","\n","fig = make_subplots(rows=1, cols=2)\n","fig.add_trace(go.Scatter(x=history.epoch, y=history.history['acc'],\n","                    mode='lines',\n","                    name='training acc'),\n","                    row=1,\n","                    col=1)\n","fig.add_trace(go.Scatter(x=history.epoch, y=history.history['val_acc'],\n","                    mode='lines',\n","                    name='validation acc'),\n","                    row=1,\n","                    col=1)\n","fig.add_trace(go.Scatter(x=history.epoch, y=history.history['loss'],\n","                    mode='lines',\n","                    name='training loss'),\n","                    row=1,\n","                    col=2)\n","fig.add_trace(go.Scatter(x=history.epoch, y=history.history['val_loss'],\n","                    mode='lines',\n","                    name='validation loss'),\n","                    row=1,\n","                    col=2)"],"execution_count":0,"outputs":[]}]}